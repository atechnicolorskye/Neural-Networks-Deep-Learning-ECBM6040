{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECBM E6040 Homework 4 - Programming Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem a: Recurrent Neural Network\n",
    "#### Bullet 1: Experiment with RNNSLU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_rnnslu(win=7, nhidden=200, emb_dimension=100, normal=True)\n",
    "test_rnnslu(win=7, nhidden=200, emb_dimension=50, normal=True)\n",
    "test_rnnslu(win=7, nhidden=400, emb_dimension=100, normal=True)\n",
    "test_rnnslu(win=7, nhidden=400, emb_dimension=50, normal=True)\n",
    "test_rnnslu(win=5, nhidden=200, emb_dimension=100, normal=True)\n",
    "test_rnnslu(win=5, nhidden=200, emb_dimension=50, normal=True)\n",
    "test_rnnslu(win=5, nhidden=400, emb_dimension=100, normal=True)\n",
    "test_rnnslu(win=5, nhidden=400, emb_dimension=50, normal=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7 200 100 True\n",
    "BEST RESULT: epoch 59 valid F1 97.16 best test F1 94.02 with the model ../result\n",
    "\n",
    "7 200 50 True\n",
    "BEST RESULT: epoch 57 valid F1 97.23 best test F1 94.2 with the model ../result\n",
    "\n",
    "7 400 100 True\n",
    "BEST RESULT: epoch 47 valid F1 97.32 best test F1 94.28 with the model ../result\n",
    "\n",
    "7 400 50 True\n",
    "BEST RESULT: epoch 50 valid F1 96.92 best test F1 93.93 with the model ../result\n",
    "\n",
    "5 200 100 True\n",
    "BEST RESULT: epoch 34 valid F1 97.01 best test F1 94.09 with the model ../result\n",
    "\n",
    "5 200 50 True\n",
    "BEST RESULT: epoch 57 valid F1 96.96 best test F1 93.57 with the model ../result\n",
    "\n",
    "5 400 100 True\n",
    "BEST RESULT: epoch 48 valid F1 97.31 best test F1 94.01 with the model ../result\n",
    "\n",
    "5 400 50 True\n",
    "BEST RESULT: epoch 25 valid F1 96.91 best test F1 93.19 with the model ../result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, it can be seen that the effect of changing the size of the context window, number of hidden units and embedding dimensions is minimal. However, there are some edge cases: in the case of context window = 7 and number of hidden units  = 200 does having fewer embedding dimensions affect the test results negatively and in the case of context window = 7 and embebdding dimension = 100 does increasing the number of hidden units affect the test results positively. \n",
    "\n",
    "In general, it can be concluded that having larger context windows, more hidden units and larger embedding dimensions leads to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bullet 2: Experiment with RNNSLU without normalizing word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_rnnslu(win=7, nhidden=200, emb_dimension=100, normal=False)\n",
    "test_rnnslu(win=7, nhidden=200, emb_dimension=50, normal=False)\n",
    "test_rnnslu(win=7, nhidden=400, emb_dimension=100, normal=False)\n",
    "test_rnnslu(win=7, nhidden=400, emb_dimension=50, normal=False)\n",
    "test_rnnslu(win=5, nhidden=200, emb_dimension=100, normal=False)\n",
    "test_rnnslu(win=5, nhidden=200, emb_dimension=50, normal=False)\n",
    "test_rnnslu(win=5, nhidden=400, emb_dimension=100, normal=False)\n",
    "test_rnnslu(win=5, nhidden=400, emb_dimension=50, normal=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7 200 100 False\n",
    "BEST RESULT: epoch 52 valid F1 97.49 best test F1 94.14 with the model ../result\n",
    "\n",
    "7 200 50 False\n",
    "BEST RESULT: epoch 31 valid F1 97.56 best test F1 94.0 with the model ../result\n",
    "\n",
    "7 400 100 False\n",
    "BEST RESULT: epoch 37 valid F1 97.32 best test F1 94.14 with the model ../result\n",
    "\n",
    "7 400 50 False\n",
    "BEST RESULT: epoch 50 valid F1 97.13 best test F1 94.09 with the model ../result\n",
    "\n",
    "5 400 100 False\n",
    "BEST RESULT: epoch 49 valid F1 97.37 best test F1 94.4 with the model ../result\n",
    "\n",
    "5 400 50 False\n",
    "BEST RESULT: epoch 43 valid F1 97.57 best test F1 93.42 with the model ../result\n",
    "\n",
    "5 200 100 False\n",
    "BEST RESULT: epoch 23 valid F1 97.34 best test F1 94.0 with the model ../result\n",
    "\n",
    "5 200 50 False\n",
    "BEST RESULT: epoch 38 valid F1 97.52 best test F1 93.89 with the model ../result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments demonstrate that the effect of changing the size of the context window, number of hidden units and embedding dimensions is minimal for unnormalised data too. However, in the case of context window = 5 and embedding dimension = 50 does having more hidden units affect the test results negatively. An interesting point is that the best result is acheived with a context window of 5 which was unexpected. \n",
    "\n",
    "Generally, normalisation led slightly poorer performance but the numbers are actually still pretty similar which was expected. Normalisation constrains all embeddings on a unit ball and discards information in the process which probably explains why unnormalised results are generally better. Like the previous set of normalised results, having larger context windows, more hidden units and larger embedding dimensions usually leads to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bullet 3: Implement RNN with two hidden layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_rnnslu2(win=7, nhidden1=200, nhidden2=200, emb_dimension=100)\n",
    "test_rnnslu2(win=7, nhidden1=200, nhidden2=200, emb_dimension=50)\n",
    "test_rnnslu2(win=7, nhidden1=200, nhidden2=400, emb_dimension=100)\n",
    "test_rnnslu2(win=7, nhidden1=200, nhidden2=400, emb_dimension=50)\n",
    "test_rnnslu2(win=7, nhidden1=400, nhidden2=200, emb_dimension=100)\n",
    "test_rnnslu2(win=7, nhidden1=400, nhidden2=200, emb_dimension=50)\n",
    "test_rnnslu2(win=7, nhidden1=400, nhidden2=400, emb_dimension=100)\n",
    "test_rnnslu2(win=7, nhidden1=400, nhidden2=400, emb_dimension=50)\n",
    "test_rnnslu2(win=5, nhidden1=200, nhidden2=200, emb_dimension=100)\n",
    "test_rnnslu2(win=5, nhidden1=200, nhidden2=200, emb_dimension=50)\n",
    "test_rnnslu2(win=5, nhidden1=200, nhidden2=400, emb_dimension=100)\n",
    "test_rnnslu2(win=5, nhidden1=200, nhidden2=400, emb_dimension=50)\n",
    "test_rnnslu2(win=5, nhidden1=400, nhidden2=200, emb_dimension=100)\n",
    "test_rnnslu2(win=5, nhidden1=400, nhidden2=200, emb_dimension=50)\n",
    "test_rnnslu2(win=5, nhidden1=400, nhidden2=400, emb_dimension=100)\n",
    "test_rnnslu2(win=5, nhidden1=400, nhidden2=400, emb_dimension=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7 200 200 100\n",
    "BEST RESULT: epoch 29 valid F1 96.17 best test F1 93.44 with the model ../result\n",
    "\n",
    "7 200 200 50\n",
    "BEST RESULT: epoch 47 valid F1 96.45 best test F1 92.78 with the model ../result\n",
    "\n",
    "7 200 400 100\n",
    "BEST RESULT: epoch 56 valid F1 96.91 best test F1 93.4 with the model ../result\n",
    "\n",
    "7 200 400 50\n",
    "BEST RESULT: epoch 46 valid F1 96.45 best test F1 93.01 with the model ../result\n",
    "\n",
    "7 400 200 100\n",
    "BEST RESULT: epoch 58 valid F1 97.09 best test F1 93.5 with the model ../result\n",
    "\n",
    "7 400 200 50\n",
    "BEST RESULT: epoch 49 valid F1 96.65 best test F1 93.14 with the model ../result\n",
    "\n",
    "7 400 400 100\n",
    "BEST RESULT: epoch 44 valid F1 96.61 best test F1 93.61 with the model ../result\n",
    "\n",
    "7 400 400 50\n",
    "BEST RESULT: epoch 25 valid F1 96.84 best test F1 92.76 with the model ../result\n",
    "\n",
    "5 200 200 100\n",
    "BEST RESULT: epoch 47 valid F1 96.73 best test F1 93.54 with the model ../result\n",
    "\n",
    "5 200 200 50\n",
    "BEST RESULT: epoch 57 valid F1 96.64 best test F1 92.85 with the model ../result\n",
    "\n",
    "5 200 400 100\n",
    "BEST RESULT: epoch 27 valid F1 96.58 best test F1 92.78 with the model ../result\n",
    "\n",
    "5 200 400 50\n",
    "BEST RESULT: epoch 49 valid F1 96.73 best test F1 93.62 with the model ../result\n",
    "\n",
    "5 400 200 100\n",
    "BEST RESULT: epoch 59 valid F1 96.85 best test F1 93.22 with the model ../result\n",
    "\n",
    "5 400 200 50\n",
    "BEST RESULT: epoch 51 valid F1 96.97 best test F1 93.5 with the model ../result\n",
    "\n",
    "5 400 400 100\n",
    "BEST RESULT: epoch 49 valid F1 96.66 best test F1 93.35 with the model ../result\n",
    "\n",
    "5 400 400 50\n",
    "BEST RESULT: epoch 37 valid F1 96.65 best test F1 93.51 with the model ../result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from above, having larger context windows, more hidden units and large embedding leads to better results. It is also obvious that having more hidden units in the first layer provides better results compared to having more hidden units in the second layer which demonstrates the first layer of processing is more important than the second layer. \n",
    "\n",
    "However, 2-layer RNNs unexpectedly performed worse than single-layer RNNs. Since 2-layer RNNs have more expressive pwoer due to them containing an additional layer, they should have performed better. The poorer performance could be explained by the low patience and small numbers of epochs provided; if the neural networks were allowed to train for longer, they might yield better results. Also, the 2-layer RNNs would also probab;y require more training data to perform better than single-layer RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bullet 4: Experiment with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "Using pdb, set a trace right after dic['words2idx'] is created to obtain the indices of the words of interest. Since the dataset contains flight information, an intuitive thing to do would be to find the differences between days of the week and compare it to other related pairs. \n",
    "\n",
    "The following words were used in the comparison experiments:\n",
    "1. {'monday': 317, 'tuesday': 518, wednesday': 546, 'thursday': 496, 'friday': 206, 'saturday': 416, 'sunday': 463}\n",
    "2. {'tuesdays': 519, 'wednesdays': 547, 'thursdays': 497,  'saturdays': 417} \n",
    "3. {'morning': 321, 'afternoon': 20, 'night': 335}\n",
    "4. {'afternoon': 20, 'noontime': 342, 'noon': 341}\n",
    "5. {'if': 233, 'different': 149, 'make': 293}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparisons between days of the week:\n",
    "'monday' - 'tuesday' = 0.51049513928794354\n",
    "'tuesday' - 'wednesday' = 0.60854357967629114\n",
    "'wednesday' - 'thursday' = 0.53786268167732931\n",
    "'thursday' - 'friday' = 0.68471195078461899\n",
    "'friday' - 'saturday' = 0.53138296565769361\n",
    "'saturday' - 'sunday' = 0.46885736271191258\n",
    "'sunday' - 'monday' = 0.49557367941104397\n",
    "\n",
    "Comparisons between same days of the week, with and without an s at the end:\n",
    "'tuesdays' - 'tuesday' = 0.95072923722295954\n",
    "'wednesdays' - 'wednesday' = 0.90264142511744727\n",
    "'thursdays' - 'thursday' = 1.1282710856894818\n",
    "'saturdays' - 'saturday' = 0.95194976117272123\n",
    "\n",
    "Comparisons between times of the day:\n",
    "'morning' - 'afternoon' = 0.62187463421791778\n",
    "'afternoon' - 'night' = 0.71985414067575793\n",
    "'night' - 'morning' = 0.69704457075112558    \n",
    "\n",
    "Comparisons between same times of the day but with different names:\n",
    "'afternoon' - 'noontime' = 1.4406427575504261\n",
    "'noontime' - 'noon' = 1.199223025161257\n",
    "'noon' - 'afternoon' = 1.4406427575504261\n",
    "\n",
    "Comparisons between days of the week and time:\n",
    "'monday' - 'morning' = 1.6368382522223417\n",
    "'tuesday' - 'afternoon' = 1.5233220059172137\n",
    "'wednesday' - 'night' = 1.5196316615463261\n",
    "\n",
    "Random comparisons:\n",
    "'monday' - 'if' = 1.4903393108778393\n",
    "'if' - 'noon' = 1.4675491832281307\n",
    "'different' - 'make' =  1.3865944145466964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the differences between the difference between related entities within ~0.1 of each other and they were similar across the board (see comparisons between days of the week and time). Within the set of days of the week, there is a much smaller difference between 'saturday' and 'sunday' as they are days of the weekend, but 'sunday' and 'monday' are also very close in space.\n",
    "\n",
    "An interesting note is that the differences between unrelated words, usually around ~1.4, are also quite similar which makes sense as there lack of relation applies across the board. However, the differences between same days of the week with and without an s at the end are much larger than differences between different days and the comparisons between the sames times of the day were greater than 1, both which were unexpected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem b: Learning Parity Function\n",
    "#### Bullet 1: Shallow MLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parameters: \n",
    "- learning_rate=0.05\n",
    "- L1_reg=0.00\n",
    "- L2_reg=0.00 \n",
    "- n_epochs=150000\n",
    "- training_batch_size=1000 \n",
    "- validation_batch_size=500\n",
    "- test_batch_size=100\n",
    "- n_hidden=300 \n",
    "- n_hiddenLayers=1 \n",
    "- patience=150000\n",
    "\n",
    "For 8 bits:\n",
    "On CPU:\n",
    "Best validation score of 0.000000 % obtained at iteration 101333, with test performance 0.000000 %\n",
    "The training process for function test_mlp_parity ran for 61.01m\n",
    "\n",
    "For 12 bits:\n",
    "On CPU:\n",
    "Best validation score of 34.400000 % obtained at iteration 136638, with test performance 37.000000 %\n",
    "The training process for function test_mlp_parity ran for 45.83m\n",
    "\n",
    "On GPU:\n",
    "Changed training examples to 10000, validation examples to 1500, testing examples to 300, validation batch size to 1500, test_batch_size to 300. Held the rest constant.\n",
    "Best validation score of 0.000000 % obtained at iteration 605840, with test performance 0.000000 %\n",
    "The training process for function test_mlp_parity ran for 51.86m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results demonstrate that 2-layer MLPs are able learn parity functions but not very efficiently. The key to obtaining 100% test accuracy is to provide the MLP enough patience, epochs and examples to learn the function. Large number of hidden units were actually not required. Regularisation was not required as data was clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bullet 2: Deep MLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parameters: \n",
    "- learning_rate=0.05\n",
    "- L1_reg=0.00\n",
    "- L2_reg=0.00 \n",
    "- n_epochs=150000\n",
    "- training_batch_size=1000 \n",
    "- validation_batch_size=500\n",
    "- test_batch_size=100\n",
    "- n_hidden=[200, 200] \n",
    "- n_hiddenLayers=2 \n",
    "- patience=150000\n",
    "\n",
    "GPU only:\n",
    "For 8 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 34950, with test performance 0.000000 %\n",
    "The training process for function test_mlp_parity ran for 9.18m\n",
    "\n",
    "For 12 bits:\n",
    "Best validation score of 16.200000 % obtained at iteration 62256, with test performance 22.000000 \n",
    "The training process for function test_mlp_parity ran for 9.19m\n",
    "\n",
    "Changed training examples to 10000, validation examples to 1500, testing examples to 300, validation batch size to 1500, test_batch_size to 300. Held the rest constant.\n",
    "Best validation score of 0.000000 % obtained at iteration 89090, with test performance 1.000000 %\n",
    "The training process for function test_mlp_parity ran for 13.21m\n",
    "\n",
    "Increased n_hidden to [200, 200, 200, 200] and n_hiddenLayers=4\n",
    "Best validation score of 0.000000 % obtained at iteration 38120, with test performance 0.000000\n",
    "The training process for function test_mlp_parity ran for 17.68m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results demonstrate that deeper MLPs are able learn parity functions much more efficiently than 2-layer MLPs. The key to obtaining 100% test accuracy is to provide the MLP enough patience, epochs and examples to learn the function. Since not many hidden units were required to learn parity functions with 2-layer MLPs, it is undestandable that less units per layer were required in deeper MLPs. Regularisation was not required as data was clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bullet 3: RNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With intermediate results obtained through changing the dataset (test_rnn_parity_all_y):\n",
    "Parameters:\n",
    "- learning_rate=0.05\n",
    "- n_hidden=12\n",
    "- n_epochs=400\n",
    "- patience=50000 \n",
    "- batch_size=1\n",
    "\n",
    "For 8 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 51000, with test performance 0.000000 %\n",
    "The training process for function test_rnn_parity_all_y ran for 1.27m    \n",
    "For 12 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 85000, with test performance 0.000000 %\n",
    "The training process for function test_rnn_parity_all_y ran for 2.43m\n",
    "\n",
    "Without intermdiate results (test_rnn_parity):\n",
    "Parameters:\n",
    "- learning_rate= 0.75\n",
    "- n_hidden=20\n",
    "- n_epochs=100000\n",
    "- training_batch_size=1000 \n",
    "- validation_batch_size=500\n",
    "- test_batch_size=100\n",
    "- patience=100000\n",
    "\n",
    "For 3 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 22450, with test performance 0.000000 %\n",
    "The training process for function test_rnn_parity ran for 11.85m\n",
    "\n",
    "Decreased n_epochs to 10000, reused weights from 3 bits\n",
    "\n",
    "For 8 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 3083, with test performance 0.000000 %\n",
    "The training process for function test_rnn_parity ran for 2.82m\n",
    "\n",
    "Decreased n_epochs to 1000, reused weights from 8 bit\n",
    "\n",
    "For 12 bits:\n",
    "Best validation score of 0.000000 % obtained at iteration 59, with test performance 0.000000 %\n",
    "The training process for function test_rnn_parity ran for 0.40m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods weere tried for this part. One was a hack for the many to many architecture as used previously where an output y was provided at each input of a bit through changing the data generation process. The other was a many to one architecture that only provided an output y after n-bits. The first method was very quick to train as much more information was provided to the network. Transfer learning was used in the second method as training time was much longer compared to the first method. The exponential decrease in the time required to train the network demonstrates the effectiveness of transfer learning in neural networks. \n",
    "\n",
    "The results demonstrate that RNNs are able to learn parity functions more efficiently compared to MLP and deep MLPs and with less neurons. Regularisation was not required as data is clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
